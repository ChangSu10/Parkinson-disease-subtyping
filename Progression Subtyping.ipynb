{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation of our PD progression subtyping method on PPMI cohort from AMP-PD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this section as setup step.\n",
    "\n",
    "You may occassionally need to re-run this notebook to re-install or update software if the notebook runtime environment changes or is restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pandas==0.24.2 pandas-gbq six>=1.13.0  # Install Pandas\n",
    "!pip3 install numpy  # Install Numpy\n",
    "!pip3 install matplotlib  # Install Matplotlib\n",
    "!pip3 install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html  # Install PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('facets'):\n",
    "    !git clone https://github.com/pair-code/facets.git\n",
    "else:\n",
    "    print('Facets already installed.')\n",
    "\n",
    "!jupyter nbextension install facets/facets-dist/ --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will extract the data from Google BigQuery into a dataframe and then impute and normalize the extracted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing the code in this section, be sure to have run the section Py3 - Install software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "import pickle as pkl\n",
    "import json\n",
    "from firecloud import api as fapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup utility classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data to be extracted can be divided into static data and longitude data. Most static data are come from the demographic information, such as age, gender, race and so on. The longitude data are come from the others category dataset, such as motor clinical assessments, non-motor clinical assessments and so on.\n",
    "\n",
    "There are four cohort can be explored in this study, they are PPMI, PDBP, HBS and BioFIND. The static data could contain the demographic inforamtion and the family history information. The longitude data could contain the data from the MDS-UPDRS Part I - IV, Schwab and England ADL scale, Montreal Cognitive Assessment (MoCA), University of Pennsylvania Smell Identification Test (UPSIT), REM Sleep Behavior Disorder Questionnaire, Epworth Sleepiness Scale, \n",
    "medical records and the biospecimen.\n",
    "\n",
    "Class \"DataPreparation\" will generate the static data and longitude data from the selected cohort and data table in the study. As well as the patients' visit information and features for futher process will be also generated.\n",
    "\n",
    "Then, the function \"concatenate\" will convert the generated longitude data into the format for feeding LSTM model. Function \"interpolate_imputation\" and \"LOCF_FOCB_imputation\" are used to impute the converted data, and funtion \"Z_score_normalization\" and \"minmax_normalization\" are used to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation():\n",
    "    \n",
    "    def __init__(self, cohort_name, table_list):\n",
    "        \n",
    "        self.table_list = table_list\n",
    "        \n",
    "        self.BILLING_PROJECT_ID = os.environ['GOOGLE_PROJECT']\n",
    "        WORKSPACE_NAMESPACE = os.environ['WORKSPACE_NAMESPACE']\n",
    "        WORKSPACE_NAME = os.environ['WORKSPACE_NAME']\n",
    "\n",
    "        WORKSPACE_ATTRIBUTES = fapi.get_workspace(WORKSPACE_NAMESPACE, WORKSPACE_NAME).json().get('workspace',{}).get('attributes',{})\n",
    "        WORKSPACE_ENTITIES = fapi.get_entities_with_type(WORKSPACE_NAMESPACE, WORKSPACE_NAME).json()\n",
    "        \n",
    "        COHORT_NAME = cohort_name\n",
    "        cohort_data = None\n",
    "        for entity in WORKSPACE_ENTITIES:\n",
    "            if entity['entityType'] != 'cohort':\n",
    "                continue\n",
    "            if entity['name'] == COHORT_NAME:\n",
    "                cohort_data = entity\n",
    "                break\n",
    "\n",
    "        assert cohort_data, f\"STOP! '{COHORT_NAME}' not found, please follow instructions above to create a cohort with this name\"\n",
    "\n",
    "        cohort_query = cohort_data['attributes']['query']\n",
    "        \n",
    "        cohort_df = self.bq_query(cohort_query)\n",
    "        pid_list = cohort_df['participant_id'].values.tolist()\n",
    "        self.patient_list = pid_list\n",
    "        \n",
    "        self.participant_df = self.bq_query(f'''\n",
    "        SELECT d.*, h.* EXCEPT(participant_id)\n",
    "        FROM `amp-pd-research.2019_v1release_1015.Demographics` AS d\n",
    "        JOIN `amp-pd-research.2019_v1release_1015.amp_pd_case_control` AS h\n",
    "        USING (participant_id)\n",
    "        WHERE d.participant_id IN ({cohort_query})''')\n",
    "        \n",
    "        history_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.Family_History_PD`''')\n",
    "        updrs1_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.MDS_UPDRS_Part_I`''')\n",
    "        updrs2_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.MDS_UPDRS_Part_II`''')\n",
    "        updrs3_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.MDS_UPDRS_Part_III`''')\n",
    "        updrs4_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.MDS_UPDRS_Part_IV`''')\n",
    "        schwab_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.Modified_Schwab___England_ADL`''')\n",
    "        moca_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.MOCA`''')\n",
    "        upsit_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.UPSIT`''')\n",
    "        ess_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.Epworth_Sleepiness_Scale`''')\n",
    "        rbd_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.REM_Sleep_Behavior_Disorder_Questionnaire_Stiasny_Kolster`''')\n",
    "        medication_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.PD_Medical_History`''')\n",
    "        bio_df = self.bq_query(f'''SELECT * FROM `amp-pd-research.2019_v1release_1015.Biospecimen_analyses_CSF_abeta_tau_ptau`''')\n",
    "\n",
    "        self.history_df = history_df.loc[history_df['participant_id'].isin(pid_list)]\n",
    "        self.updrs1_df = updrs1_df.loc[updrs1_df['participant_id'].isin(pid_list)]\n",
    "        self.updrs2_df = updrs2_df.loc[updrs2_df['participant_id'].isin(pid_list)]\n",
    "        self.updrs3_df = updrs3_df.loc[updrs3_df['participant_id'].isin(pid_list)]\n",
    "        self.updrs4_df = updrs4_df.loc[updrs4_df['participant_id'].isin(pid_list)]\n",
    "        self.schwab_df = schwab_df.loc[schwab_df['participant_id'].isin(pid_list)]\n",
    "        self.moca_df = moca_df.loc[moca_df['participant_id'].isin(pid_list)]\n",
    "        self.upsit_df = upsit_df.loc[upsit_df['participant_id'].isin(pid_list)]\n",
    "        self.ess_df = ess_df.loc[ess_df['participant_id'].isin(pid_list)]\n",
    "        self.rbd_df = rbd_df.loc[rbd_df['participant_id'].isin(pid_list)]\n",
    "        self.medication_df = medication_df.loc[medication_df['participant_id'].isin(pid_list)]\n",
    "        self.bio_df = bio_df.loc[bio_df['participant_id'].isin(pid_list)]\n",
    "        \n",
    "        demographics = self.extract_demographics()\n",
    "        history = self.extract_history()\n",
    "        updrs1 = self.extract_updrs1()\n",
    "        updrs2 = self.extract_updrs2()\n",
    "        updrs3 = self.extract_updrs3()\n",
    "        updrs4 = self.extract_updrs4()\n",
    "        schwab = self.extract_schwab()\n",
    "        moca = self.extract_moca()\n",
    "        upsit = self.extract_upsit()\n",
    "        ess = self.extract_ess()\n",
    "        rbd = self.extract_rbd()\n",
    "        med = self.extract_medication()\n",
    "        bio = self.extract_bio()\n",
    "        \n",
    "        self.table_dict = {\"demographics\": demographics, \"history\": history, \"updrs1\": updrs1, \"updrs2\": updrs2, \"updrs3\": updrs3, \"updrs4\": updrs4,\n",
    "                          \"schwab\": schwab, \"moca\": moca, \"upsit\": upsit, \"ess\": ess, \"rbd\": rbd, \"med\": med, \"bio\": bio}\n",
    "        \n",
    "        self.feature_knowledge = [\"code_upd2101_cognitive_impairment\", \"code_upd2102_hallucinations_and_psychosis\", \"code_upd2103_depressed_mood\", \n",
    "                \"code_upd2104_anxious_mood\", \"code_upd2105_apathy\", \"code_upd2106_dopamine_dysregulation_syndrome_features\", \n",
    "                \"code_upd2107_pat_quest_sleep_problems\", \"code_upd2108_pat_quest_daytime_sleepiness\", \n",
    "                \"code_upd2109_pat_quest_pain_and_other_sensations\", \"code_upd2110_pat_quest_urinary_problems\", \n",
    "                \"code_upd2111_pat_quest_constipation_problems\", \"code_upd2112_pat_quest_lightheadedness_on_standing\", \n",
    "                \"code_upd2113_pat_quest_fatigue\", \"code_upd2201_speech\", \"code_upd2202_saliva_and_drooling\", \"code_upd2203_chewing_and_swallowing\", \n",
    "                \"code_upd2204_eating_tasks\", \"code_upd2205_dressing\", \"code_upd2206_hygiene\", \"code_upd2207_handwriting\", \n",
    "                \"code_upd2208_doing_hobbies_and_other_activities\", \"code_upd2209_turning_in_bed\", \"code_upd2210_tremor\", \n",
    "                \"code_upd2211_get_out_of_bed_car_or_deep_chair\", \"code_upd2212_walking_and_balance\", \"code_upd2213_freezing\", \n",
    "                \"code_upd2301_speech_problems\", \"code_upd2302_facial_expression\", \"code_upd2303a_rigidity_neck\", \n",
    "                \"code_upd2303b_rigidity_rt_upper_extremity\", \"code_upd2303c_rigidity_left_upper_extremity\", \n",
    "                \"code_upd2303d_rigidity_rt_lower_extremity\", \"code_upd2303e_rigidity_left_lower_extremity\", \n",
    "                \"code_upd2304a_right_finger_tapping\", \"code_upd2304b_left_finger_tapping\", \"code_upd2305a_right_hand_movements\", \n",
    "                \"code_upd2305b_left_hand_movements\", \"code_upd2306a_pron_sup_movement_right_hand\", \"code_upd2306b_pron_sup_movement_left_hand\", \n",
    "                \"code_upd2307a_right_toe_tapping\", \"code_upd2307b_left_toe_tapping\", \"code_upd2308a_right_leg_agility\", \n",
    "                \"code_upd2308b_left_leg_agility\", \"code_upd2309_arising_from_chair\", \"code_upd2310_gait\", \"code_upd2311_freezing_of_gait\", \n",
    "                \"code_upd2312_postural_stability\", \"code_upd2313_posture\", \"code_upd2314_body_bradykinesia\", \n",
    "                \"code_upd2315a_postural_tremor_of_right_hand\", \"code_upd2315b_postural_tremor_of_left_hand\", \n",
    "                \"code_upd2316a_kinetic_tremor_of_right_hand\", \"code_upd2316b_kinetic_tremor_of_left_hand\", \n",
    "                \"code_upd2317a_rest_tremor_amplitude_right_upper_extremity\", \"code_upd2317b_rest_tremor_amplitude_left_upper_extremity\", \n",
    "                \"code_upd2317c_rest_tremor_amplitude_right_lower_extremity\", \"code_upd2317d_rest_tremor_amplitude_left_lower_extremity\", \n",
    "                \"code_upd2317e_rest_tremor_amplitude_lip_or_jaw\", \"code_upd2318_consistency_of_rest_tremor\", \n",
    "                \"mod_schwab_england_pct_adl_score\", \n",
    "                \"moca01_alternating_trail_making\", \"moca02_visuoconstr_skills_cube\", \"moca03_visuoconstr_skills_clock_cont\", \n",
    "                \"moca04_visuoconstr_skills_clock_num\", \"moca05_visuoconstr_skills_clock_hands\", \"moca06_naming_lion\", \n",
    "                \"moca07_naming_rhino\", \"moca08_naming_camel\", \"moca09_attention_forward_digit_span\", \n",
    "                \"moca10_attention_backward_digit_span\", \"moca11_attention_vigilance\", \"moca12_attention_serial_7s\", \n",
    "                \"moca13_sentence_repetition\", \"moca14_verbal_fluency_number_of_words\", \"moca15_verbal_fluency\", \n",
    "                \"moca16_abstraction\", \"moca17_delayed_recall_face\", \"moca18_delayed_recall_velvet\", \"moca19_delayed_recall_church\", \n",
    "                \"moca20_delayed_recall_daisy\", \"moca21_delayed_recall_red\", \"moca22_orientation_date_score\", \n",
    "                \"moca23_orientation_month_score\", \"moca24_orientation_year_score\", \"moca25_orientation_day_score\", \n",
    "                \"moca26_orientation_place_score\", \"moca27_orientation_city_score\", \"moca_total_score\", \n",
    "                \"code_ess0101_sitting_and_reading\", \"code_ess0102_watching_tv\", \"code_ess0103_sitting_inactive_in_public_place\", \n",
    "                \"code_ess0104_passenger_in_car_for_hour\", \"code_ess0105_lying_down_to_rest_in_afternoon\", \n",
    "                \"code_ess0106_sitting_and_talking_to_someone\", \"code_ess0107_sitting_after_lunch\", \"code_ess0108_car_stopped_in_traffic\", \n",
    "                \"code_rbd01_vivid_dreams\", \"code_rbd02_aggressive_or_action_packed_dreams\", \"code_rbd03_nocturnal_behaviour\", \n",
    "                \"code_rbd04_move_arms_legs_during_sleep\", \"code_rbd05_hurt_bed_partner\", \"code_rbd06_1_speaking_in_sleep\", \n",
    "                \"code_rbd06_2_sudden_limb_movements\", \"code_rbd06_3_complex_movements\", \"code_rbd06_4_things_fell_down\", \n",
    "                \"code_rbd07_my_movements_awake_me\", \"code_rbd08_remember_dreams\", \"code_rbd09_sleep_is_disturbed\", \n",
    "                \"code_rbd10a_stroke\", \"code_rbd10b_head_trauma\", \"code_rbd10c_parkinsonism\", \"code_rbd10d_rls\", \"code_rbd10e_narcolepsy\", \n",
    "                \"code_rbd10f_depression\", \"code_rbd10g_epilepsy\", \"code_rbd10h_brain_inflammatory_disease\", \"code_rbd10i_other\", \n",
    "                \"Tau\", \"Abeta\", \"p-Tau\"]\n",
    "    \n",
    "    def query(self):\n",
    "        new_dict = {}\n",
    "        for table_name in self.table_list:\n",
    "            new_dict[table_name] = self.table_dict[table_name]\n",
    "        return new_dict\n",
    "            \n",
    "    def static_data(self, save_path=None):\n",
    "        if save_path is not None:\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_path)\n",
    "            with open(save_path + 'static_info.csv', 'w') as f:\n",
    "                f.write(self.participant_df.to_csv(index=False))\n",
    "        return self.participant_df\n",
    "    \n",
    "    def longitudinal_data(self, save_path=None):\n",
    "        selected_table = self.query()\n",
    "        res_df = pd.DataFrame()\n",
    "        feature_list = []\n",
    "        for table_name in selected_table:\n",
    "            temp_feature_list = list(selected_table[table_name].drop_duplicates(subset='variable', keep='first')['variable'].values)\n",
    "            feature_list = feature_list + temp_feature_list\n",
    "            res_df = pd.concat([res_df, selected_table[table_name]], ignore_index=True)\n",
    "            \n",
    "        res_df = res_df.replace(\"Yes\", 1)\n",
    "        res_df = res_df.replace(\"No\", 0)\n",
    "        res_df = res_df.replace(\"None\", np.nan)\n",
    "        feature_list = list(set(self.feature_knowledge) & set(feature_list))\n",
    "        \n",
    "        if save_path is not None:\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_path)\n",
    "            with open(save_path + 'data.csv', 'w') as f:\n",
    "                f.write(res_df.to_csv(index=False))\n",
    "            with open(save_path + \"feature_list.txt\", \"wb\") as fp:\n",
    "                pkl.dump(feature_list, fp)\n",
    "\n",
    "        return res_df, feature_list\n",
    "    \n",
    "    def visit_list(self, save_path=None):\n",
    "        visit_total = []\n",
    "        for table_name in self.table_dict:\n",
    "            visit_list = self.table_dict[table_name].drop_duplicates(subset='visit_name', keep='first')['visit_name'].values.tolist()\n",
    "            visit_total = list(set(visit_total + visit_list))\n",
    "        visit_total.sort(key=self.natural_keys)\n",
    "        visit_total.remove(\"LOG\")\n",
    "        visit_total.remove(\"SC\")\n",
    "        visit_total.remove(\"SC#2\")\n",
    "        if save_path is not None:\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_path)\n",
    "            with open(save_path + \"visit_list.txt\", \"wb\") as fp:\n",
    "                pkl.dump(visit_total, fp)\n",
    "                \n",
    "        return visit_total\n",
    "    \n",
    "    def visit_info(self, data, visit_list, save_path=None):  \n",
    "        \n",
    "        visit_info_df = pd.DataFrame(self.patient_list, columns=[\"participant_id\"])\n",
    "        visit_info_df = visit_info_df.reindex(columns=['participant_id'] + visit_list + ['max_visit'])\n",
    "        visit_info_df.set_index(['participant_id'], inplace = True)\n",
    "\n",
    "        for idx, row in data.iterrows():\n",
    "            pid, visit_name, var_type = row['participant_id'], row['visit_name'], row['var_type']\n",
    "            if visit_name in visit_info_df.columns:\n",
    "                visit_info_df.loc[pid, visit_name] = 1\n",
    "\n",
    "        visit_info_df = visit_info_df.reset_index()\n",
    "\n",
    "        for idx, row in visit_info_df.iterrows():\n",
    "            max_visit = None\n",
    "            for v in visit_list:\n",
    "                if not np.isnan(row[v]):\n",
    "                    max_visit = v\n",
    "                visit_info_df.loc[idx, \"max_visit\"] = max_visit\n",
    "        if save_path is not None:\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_path)\n",
    "            with open(save_path + 'visit_info.csv', 'w') as f:\n",
    "                f.write(visit_info_df.to_csv(index=False))\n",
    "\n",
    "        return visit_info_df\n",
    "    \n",
    "    def concatenate(self, data_df, visit_info, visit_list, feature_list, save_path=None):\n",
    "        \n",
    "        visit_id_map = {}\n",
    "        for v in range(len(visit_list)):\n",
    "            visit_id_map[visit_list[v]] = v\n",
    "            \n",
    "        patient_length = {}\n",
    "        for idx, row in visit_info.iterrows():\n",
    "            pid, BL, max_visit = row['participant_id'], row['M0'], row['max_visit']\n",
    "            if np.isnan(BL):  # check is there missing of BL visit, if so, exclude the patient\n",
    "                print(\"!!!! Patient %s has no BL information!\", pid)\n",
    "            else:\n",
    "                patient_length[pid] = visit_id_map[max_visit]+1\n",
    "\n",
    "        data_df = data_df.loc[data_df[\"participant_id\"].isin(list(patient_length.keys()))]\n",
    "        data_df = data_df.loc[data_df[\"visit_name\"].isin(visit_list)]\n",
    "        \n",
    "        M = len(feature_list)\n",
    "\n",
    "        # initialize sequential data of each patient\n",
    "        patient_arrays = {}  # key: PATNO, value: numpy ndarray (column: feature, row: visit)\n",
    "        for p in patient_length:\n",
    "            n = patient_length[p]\n",
    "            patient_arrays[p] = np.full((n, M), np.nan)\n",
    "\n",
    "        # create feature_id map\n",
    "        feature_id_map = {}  # key: feature name, value: f_id (int)\n",
    "        for f_id in range(len(feature_list)):\n",
    "            feature_id_map[feature_list[f_id]] = f_id\n",
    "        # print(feature_id_map)\n",
    "\n",
    "        # update the data matrices of patients\n",
    "        data_df = data_df[data_df['variable'].isin(feature_id_map)]  # exclude unused features\n",
    "        for idx, row in data_df.iterrows():\n",
    "            pid, visit_name, variable, value = row['participant_id'], row['visit_name'], row['variable'], row['value']\n",
    "            v_id = visit_id_map[visit_name]\n",
    "            f_id = feature_id_map[variable]\n",
    "            patient_arrays[pid][v_id, f_id] = value\n",
    "\n",
    "        # compute feature median\n",
    "        patient_null_column = []\n",
    "        feature_median = {}  # key: feature, value: median of the feature\n",
    "        patient_feature_median = {}  # key: patient, value: { feature : median of the feature of patient }\n",
    "        for pid in patient_length:\n",
    "            patient_feature_median[pid] = {}  # initialize\n",
    "        for var in feature_list:\n",
    "            # feature median\n",
    "            temp_data = data_df[data_df['variable'] == var]\n",
    "            feature_median[var] = np.nanmedian(list(temp_data['value'].astype('float').values))\n",
    "            # patient median\n",
    "            for p in patient_feature_median:\n",
    "                patient_temp_data = temp_data[temp_data['participant_id'] == p]\n",
    "                tmp_values = list(patient_temp_data['value'].astype('float').values)\n",
    "                if (len(tmp_values) == 0) or (len(tmp_values) == sum(np.isnan(tmp_values))):  # patient do not have information of this feature\n",
    "                    patient_feature_median[p][var] = feature_median[var]\n",
    "                    patient_null_column.append(p)\n",
    "                else:\n",
    "                    patient_feature_median[p][var] = round(np.nanmedian(tmp_values))\n",
    "                    \n",
    "        if save_path is not None:\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_path)\n",
    "            with open(save_path + \"sequence_data.pkl\", \"wb\") as wf:\n",
    "                pkl.dump(patient_arrays, wf)\n",
    "            with open(save_path + \"feature_median.json\", 'w') as wf:\n",
    "                json.dump(feature_median, wf, indent=4)\n",
    "            with open(save_path + \"patient_feature_median.json\", 'w') as wf:\n",
    "                json.dump(patient_feature_median, wf, indent=4)\n",
    "                    \n",
    "        return (patient_arrays, feature_median, patient_feature_median)\n",
    "    \n",
    "    def interpolate_imputation(self, sequence_data, feature_median, feature_list, save_path=None):\n",
    "        \"\"\"\n",
    "        Imputation based on pandas' interpolate method.\n",
    "\n",
    "        :param sequence_data: key: participant_id, value: data matrix\n",
    "        :param feature_median: median of each feature\n",
    "        :param feature_list: list of features\n",
    "\n",
    "        :return: imputed_data\n",
    "        \"\"\"\n",
    "\n",
    "        M = len(feature_list)\n",
    "\n",
    "        imputed_data = {}\n",
    "\n",
    "        for p in sequence_data:\n",
    "            data = pd.DataFrame(sequence_data[p])\n",
    "\n",
    "            # address the issue that all column are nan\n",
    "            for m in range(M):\n",
    "                if data[m].isnull().all():\n",
    "                    data[m] = feature_median[feature_list[m]]\n",
    "\n",
    "            data = data.interpolate(method='linear', axis=0, limit_direction='both')\n",
    "\n",
    "            imputed_data[p] = data.values\n",
    "            \n",
    "        if save_path is not None:\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_path)\n",
    "            with open(save_path + \"sequence_data_interpolate_imputation.pkl\", \"wb\") as wf:\n",
    "                pkl.dump(imputed_data, wf)\n",
    "\n",
    "        return imputed_data\n",
    "    \n",
    "    def LOCF_FOCB_imputation(self, sequence_data, patient_feature_median, feature_list, save_path=None):\n",
    "        \"\"\"\n",
    "        LOFC: last occurrence carry forward strategy\n",
    "        FOCB: first occurrence carry backward strategy\n",
    "\n",
    "        :param sequence_data: key: participant_id, value: data matrix\n",
    "        :param patient_feature_median: median of each feature of each patient\n",
    "        :param feature_list: list of features\n",
    "\n",
    "        :return: imputed_data\n",
    "        \"\"\"\n",
    "\n",
    "        imputed_data = {}\n",
    "\n",
    "        for p in sequence_data:\n",
    "            data = sequence_data[p]\n",
    "            L, N = data.shape\n",
    "\n",
    "            # build mask matrix (0: has missing value in the location, 1: other)\n",
    "            data = (pd.DataFrame(data)).fillna(-1).values  # fill NaN as -1\n",
    "            mask_idx = np.where(data == -1)  # first row: x-axis, second row: y-axis\n",
    "            mask_matrix = np.ones((L, N), dtype='int')\n",
    "            mask_matrix[mask_idx] = 0\n",
    "\n",
    "            missing_num = len(mask_idx[0])\n",
    "            for i in range(missing_num):\n",
    "                row_idx = mask_idx[0][i]\n",
    "                col_idx = mask_idx[1][i]\n",
    "\n",
    "                if L == 1:  # only one visit\n",
    "                    data[row_idx, col_idx] = patient_feature_median[p][feature_list[col_idx]]\n",
    "\n",
    "                else:       # multiple visit\n",
    "                    if row_idx == 0:  # first visit is NaN\n",
    "\n",
    "                        if int(data[row_idx + 1, col_idx]) != -1:  # using FOCB\n",
    "                            data[row_idx, col_idx] = data[row_idx + 1, col_idx]\n",
    "                        else:  # using median of feature of patient\n",
    "                            data[row_idx, col_idx] = patient_feature_median[p][feature_list[col_idx]]\n",
    "\n",
    "                    elif row_idx == L-1:  # last visit\n",
    "                        if int(data[row_idx - 1, col_idx]) != -1:  # using LOCF\n",
    "                            data[row_idx, col_idx] = data[row_idx - 1, col_idx]\n",
    "                        else:  # using median of feature of patient\n",
    "                            data[row_idx, col_idx] = patient_feature_median[p][feature_list[col_idx]]\n",
    "\n",
    "                    else:\n",
    "                        if int(data[row_idx - 1, col_idx]) != -1:  # using LOCF\n",
    "                            data[row_idx, col_idx] = data[row_idx - 1, col_idx]\n",
    "                            continue\n",
    "                        if int(data[row_idx + 1, col_idx]) != -1:  # using FOCB\n",
    "                            data[row_idx, col_idx] = data[row_idx + 1, col_idx]\n",
    "                            continue\n",
    "                        data[row_idx, col_idx] = patient_feature_median[p][feature_list[col_idx]]\n",
    "\n",
    "            imputed_data[p] = data\n",
    "            \n",
    "        if save_path is not None:\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_path)\n",
    "            with open(save_path + \"sequence_data_LOCF_FOCB_imputation.pkl\", \"wb\") as wf:\n",
    "                pkl.dump(imputed_data, wf)\n",
    "            \n",
    "        return imputed_data\n",
    "    \n",
    "    def Z_score_normalization(self, sequence_data, feature_list, save_path=None):\n",
    "        \"\"\"\n",
    "        (value - mean) / standard deviation\n",
    "\n",
    "        :param sequence_data: key: participant_id, value: data matrix\n",
    "\n",
    "        :return: normalized_sequence_data\n",
    "        \"\"\"\n",
    "        \n",
    "        M = len(feature_list)\n",
    "        \n",
    "        patients = sequence_data.keys()\n",
    "\n",
    "        normalized_sequence_data = copy.deepcopy(sequence_data)\n",
    "\n",
    "        for m in range(M):\n",
    "            values = np.array([])  # initialize the vector of m-th feature\n",
    "            for p in patients:\n",
    "                values = np.concatenate((values, sequence_data[p][:, m])) \n",
    "            # take the mean()\n",
    "            feature_mean = values.mean()\n",
    "            # take standard deviation\n",
    "            feature_std = values.std()\n",
    "\n",
    "            # update the data with Z score\n",
    "            for p in patients:\n",
    "                Zscore = (sequence_data[p][:, m] - feature_mean) / feature_std\n",
    "                normalized_sequence_data[p][:, m] = Zscore\n",
    "                \n",
    "        if save_path is not None:\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_path)\n",
    "            with open(save_path + \"sequence_data_Zscore.pkl\", \"wb\") as wf:\n",
    "                pkl.dump(normalized_sequence_data, wf)\n",
    "\n",
    "        return normalized_sequence_data\n",
    "    \n",
    "    def minmax_normalization(self, sequence_data, feature_list, save_path=None):\n",
    "        \"\"\"\n",
    "            (value - min) / (max - min)\n",
    "\n",
    "            :param sequence_data: key: participant_id, value: data matrix\n",
    "\n",
    "            :return: (normalized) sequence_data\n",
    "            \"\"\"\n",
    "        \n",
    "        M = len(feature_list)\n",
    "        \n",
    "        patients = sequence_data.keys()\n",
    "\n",
    "        normalized_sequence_data = copy.deepcopy(sequence_data)\n",
    "\n",
    "        for m in range(M):\n",
    "            values = np.array([])  # initialize the vector of m-th feature\n",
    "            for p in patients:\n",
    "                values = np.concatenate((values, sequence_data[p][:, m]))\n",
    "            # take the min()\n",
    "            feature_min = values.min()\n",
    "            # take the max()\n",
    "            feature_max = values.max()\n",
    "\n",
    "            # update the data with min-max normalization value\n",
    "            for p in patients:\n",
    "                norm_value = (sequence_data[p][:, m] - feature_min) / (feature_max - feature_min)\n",
    "                normalized_sequence_data[p][:, m] = norm_value\n",
    "                \n",
    "        if save_path is not None:\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_path)\n",
    "            with open(save_path + \"sequence_data_minmax.pkl\", \"wb\") as wf:\n",
    "                pkl.dump(normalized_sequence_data, wf)\n",
    "\n",
    "        return normalized_sequence_data\n",
    "    \n",
    "    def atoi(self, text):\n",
    "        return int(text) if text.isdigit() else text\n",
    "    \n",
    "    def natural_keys(self, text):\n",
    "        return [self.atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "        \n",
    "    def bq_query(self, query):\n",
    "        return pd.read_gbq(query, project_id=self.BILLING_PROJECT_ID, dialect='standard')\n",
    "    \n",
    "    def extract_demographics(self):\n",
    "        features = [\"age_at_baseline\", \"sex\", \"race\", \"education_level_years\", \"diagnosis_at_baseline\", \"diagnosis_latest\"]\n",
    "        demographics = self.participant_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": demographics[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": demographics[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": demographics[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(demographics),\n",
    "                                   \"var_type\": ['subject_characteristics'] * len(demographics),\n",
    "                                   \"source\": ['demographics'] * len(demographics),\n",
    "                                   \"value\": demographics[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        return res_df\n",
    "\n",
    "    def extract_history(self):\n",
    "        features = [\"biological_mother_with_pd\", \"biological_father_with_pd\", \"other_relative_with_pd\"]\n",
    "        history = self.history_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": history[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": history[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": history[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(history),\n",
    "                                   \"var_type\": ['subject_characteristics'] * len(history),\n",
    "                                   \"source\": ['family_history'] * len(history),\n",
    "                                   \"value\": history[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        return res_df\n",
    "    \n",
    "    \n",
    "    def extract_updrs1(self):\n",
    "        features = [\"code_upd2101_cognitive_impairment\", \"code_upd2102_hallucinations_and_psychosis\", \"code_upd2103_depressed_mood\", \n",
    "                    \"code_upd2104_anxious_mood\", \"code_upd2105_apathy\", \"code_upd2106_dopamine_dysregulation_syndrome_features\", \n",
    "                    \"code_upd2107_pat_quest_sleep_problems\", \"code_upd2108_pat_quest_daytime_sleepiness\", \n",
    "                    \"code_upd2109_pat_quest_pain_and_other_sensations\", \"code_upd2110_pat_quest_urinary_problems\", \n",
    "                    \"code_upd2111_pat_quest_constipation_problems\", \"code_upd2112_pat_quest_lightheadedness_on_standing\", \n",
    "                    \"code_upd2113_pat_quest_fatigue\"]\n",
    "        updrs1 = self.updrs1_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": updrs1[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": updrs1[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": updrs1[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(updrs1),\n",
    "                                   \"var_type\": ['motor'] * len(updrs1),\n",
    "                                   \"source\": ['updrs1'] * len(updrs1),\n",
    "                                   \"value\": updrs1[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        return res_df\n",
    "    \n",
    "    \n",
    "    def extract_updrs2(self):\n",
    "        features = [\"code_upd2201_speech\", \"code_upd2202_saliva_and_drooling\", \"code_upd2203_chewing_and_swallowing\", \n",
    "                    \"code_upd2204_eating_tasks\", \"code_upd2205_dressing\", \"code_upd2206_hygiene\", \"code_upd2207_handwriting\", \n",
    "                    \"code_upd2208_doing_hobbies_and_other_activities\", \"code_upd2209_turning_in_bed\", \"code_upd2210_tremor\", \n",
    "                    \"code_upd2211_get_out_of_bed_car_or_deep_chair\", \"code_upd2212_walking_and_balance\", \"code_upd2213_freezing\"]\n",
    "        updrs2 = self.updrs2_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": updrs2[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": updrs2[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": updrs2[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(updrs2),\n",
    "                                   \"var_type\": ['motor'] * len(updrs2),\n",
    "                                   \"source\": ['updrs2'] * len(updrs2),\n",
    "                                   \"value\": updrs2[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        return res_df\n",
    "    \n",
    "    \n",
    "    def extract_updrs3(self):\n",
    "        features = [\"code_upd2301_speech_problems\", \"code_upd2302_facial_expression\", \"code_upd2303a_rigidity_neck\", \n",
    "                    \"code_upd2303b_rigidity_rt_upper_extremity\", \"code_upd2303c_rigidity_left_upper_extremity\", \n",
    "                    \"code_upd2303d_rigidity_rt_lower_extremity\", \"code_upd2303e_rigidity_left_lower_extremity\", \n",
    "                    \"code_upd2304a_right_finger_tapping\", \"code_upd2304b_left_finger_tapping\", \"code_upd2305a_right_hand_movements\", \n",
    "                    \"code_upd2305b_left_hand_movements\", \"code_upd2306a_pron_sup_movement_right_hand\", \"code_upd2306b_pron_sup_movement_left_hand\", \n",
    "                    \"code_upd2307a_right_toe_tapping\", \"code_upd2307b_left_toe_tapping\", \"code_upd2308a_right_leg_agility\", \n",
    "                    \"code_upd2308b_left_leg_agility\", \"code_upd2309_arising_from_chair\", \"code_upd2310_gait\", \"code_upd2311_freezing_of_gait\", \n",
    "                    \"code_upd2312_postural_stability\", \"code_upd2313_posture\", \"code_upd2314_body_bradykinesia\", \n",
    "                    \"code_upd2315a_postural_tremor_of_right_hand\", \"code_upd2315b_postural_tremor_of_left_hand\", \n",
    "                    \"code_upd2316a_kinetic_tremor_of_right_hand\", \"code_upd2316b_kinetic_tremor_of_left_hand\", \n",
    "                    \"code_upd2317a_rest_tremor_amplitude_right_upper_extremity\", \"code_upd2317b_rest_tremor_amplitude_left_upper_extremity\", \n",
    "                    \"code_upd2317c_rest_tremor_amplitude_right_lower_extremity\", \"code_upd2317d_rest_tremor_amplitude_left_lower_extremity\", \n",
    "                    \"code_upd2317e_rest_tremor_amplitude_lip_or_jaw\", \"code_upd2318_consistency_of_rest_tremor\"]\n",
    "        updrs3 = self.updrs3_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": updrs3[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": updrs3[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": updrs3[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(updrs3),\n",
    "                                   \"var_type\": ['motor'] * len(updrs3),\n",
    "                                   \"source\": ['updrs3'] * len(updrs3),\n",
    "                                   \"value\": updrs3[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        return res_df\n",
    "    \n",
    "    \n",
    "    def extract_updrs4(self):\n",
    "        features = [\"code_upd2401_time_spent_with_dyskinesias\", \"code_upd2402_functional_impact_of_dyskinesias\", \n",
    "                    \"code_upd2403_time_spent_in_the_off_state\", \"code_upd2404_functional_impact_of_fluctuations\", \n",
    "                    \"code_upd2405_complexity_of_motor_fluctuations\", \"code_upd2406_painful_off_state_dystonia\"]\n",
    "        updrs4 = self.updrs4_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": updrs4[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": updrs4[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": updrs4[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(updrs4),\n",
    "                                   \"var_type\": ['motor'] * len(updrs4),\n",
    "                                   \"source\": ['updrs4'] * len(updrs4),\n",
    "                                   \"value\": updrs4[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        return res_df\n",
    "    \n",
    "    \n",
    "    def extract_schwab(self):\n",
    "        features = [\"mod_schwab_england_pct_adl_score\"]\n",
    "        schwab = self.schwab_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": schwab[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": schwab[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": schwab[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(schwab),\n",
    "                                   \"var_type\": ['motor'] * len(schwab),\n",
    "                                   \"source\": ['schwab'] * len(schwab),\n",
    "                                   \"value\": schwab[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        return res_df\n",
    "    \n",
    "    \n",
    "    def extract_moca(self):\n",
    "        features = [\"moca01_alternating_trail_making\", \"moca02_visuoconstr_skills_cube\", \"moca03_visuoconstr_skills_clock_cont\", \n",
    "                    \"moca04_visuoconstr_skills_clock_num\", \"moca05_visuoconstr_skills_clock_hands\", \"moca06_naming_lion\", \n",
    "                    \"moca07_naming_rhino\", \"moca08_naming_camel\", \"moca09_attention_forward_digit_span\", \n",
    "                    \"moca10_attention_backward_digit_span\", \"moca11_attention_vigilance\", \"moca12_attention_serial_7s\", \n",
    "                    \"moca13_sentence_repetition\", \"moca14_verbal_fluency_number_of_words\", \"moca15_verbal_fluency\", \n",
    "                    \"moca16_abstraction\", \"moca17_delayed_recall_face\", \"moca18_delayed_recall_velvet\", \"moca19_delayed_recall_church\", \n",
    "                    \"moca20_delayed_recall_daisy\", \"moca21_delayed_recall_red\", \"moca22_orientation_date_score\", \n",
    "                    \"moca23_orientation_month_score\", \"moca24_orientation_year_score\", \"moca25_orientation_day_score\", \n",
    "                    \"moca26_orientation_place_score\", \"moca27_orientation_city_score\", \"moca_total_score\"]\n",
    "        moca = self.moca_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": moca[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": moca[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": moca[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(moca),\n",
    "                                   \"var_type\": ['non_motor'] * len(moca),\n",
    "                                   \"source\": ['moca'] * len(moca),\n",
    "                                   \"value\": moca[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        return res_df\n",
    "    \n",
    "    \n",
    "    def extract_upsit(self):\n",
    "        features = [\"score_from_booklet_1\", \"score_from_booklet_2\", \"score_from_booklet_3\", \"score_from_booklet_4\"]\n",
    "        upsit = self.upsit_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": upsit[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": upsit[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": upsit[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(upsit),\n",
    "                                   \"var_type\": ['non_motor'] * len(upsit),\n",
    "                                   \"source\": ['upsit'] * len(upsit),\n",
    "                                   \"value\": upsit[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        return res_df\n",
    "    \n",
    "    \n",
    "    def extract_ess(self):\n",
    "        features = [\"code_ess0101_sitting_and_reading\", \"code_ess0102_watching_tv\", \"code_ess0103_sitting_inactive_in_public_place\", \n",
    "                    \"code_ess0104_passenger_in_car_for_hour\", \"code_ess0105_lying_down_to_rest_in_afternoon\", \"code_ess0106_sitting_and_talking_to_someone\", \n",
    "                    \"code_ess0107_sitting_after_lunch\", \"code_ess0108_car_stopped_in_traffic\"]\n",
    "        ess = self.ess_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": ess[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": ess[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": ess[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(ess),\n",
    "                                   \"var_type\": ['non_motor'] * len(ess),\n",
    "                                   \"source\": ['ess'] * len(ess),\n",
    "                                   \"value\": ess[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        return res_df\n",
    "    \n",
    "    \n",
    "    def extract_rbd(self):\n",
    "        features = [\"code_rbd01_vivid_dreams\", \"code_rbd02_aggressive_or_action_packed_dreams\", \"code_rbd03_nocturnal_behaviour\", \n",
    "                    \"code_rbd04_move_arms_legs_during_sleep\", \"code_rbd05_hurt_bed_partner\", \"code_rbd06_1_speaking_in_sleep\", \n",
    "                    \"code_rbd06_2_sudden_limb_movements\", \"code_rbd06_3_complex_movements\", \"code_rbd06_4_things_fell_down\", \n",
    "                    \"code_rbd07_my_movements_awake_me\", \"code_rbd08_remember_dreams\", \"code_rbd09_sleep_is_disturbed\", \n",
    "                    \"code_rbd10a_stroke\", \"code_rbd10b_head_trauma\", \"code_rbd10c_parkinsonism\", \"code_rbd10d_rls\", \"code_rbd10e_narcolepsy\", \n",
    "                    \"code_rbd10f_depression\", \"code_rbd10g_epilepsy\", \"code_rbd10h_brain_inflammatory_disease\", \"code_rbd10i_other\"]\n",
    "        rbd = self.rbd_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": rbd[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": rbd[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": rbd[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(rbd),\n",
    "                                   \"var_type\": ['non_motor'] * len(rbd),\n",
    "                                   \"source\": ['rbd'] * len(rbd),\n",
    "                                   \"value\": rbd[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        return res_df\n",
    "    \n",
    "    \n",
    "    def extract_medication(self):\n",
    "        features = [\"on_levodopa\", \"on_dopamine_agonist\", \"on_other_pd_medications\"]\n",
    "        medication = self.medication_df[[\"participant_id\", \"visit_name\", \"visit_month\"] + features]\n",
    "        res_df = pd.DataFrame()\n",
    "        for f in features:\n",
    "            temp_df = pd.DataFrame({\"participant_id\": medication[\"participant_id\"].values.tolist(), \n",
    "                                   \"visit_name\": medication[\"visit_name\"].values.tolist(),\n",
    "                                   \"visit_month\": medication[\"visit_month\"].values.tolist(),\n",
    "                                   \"variable\": [f] * len(medication),\n",
    "                                   \"var_type\": ['medical'] * len(medication),\n",
    "                                   \"source\": ['medication'] * len(medication),\n",
    "                                   \"value\": medication[f].values.tolist()})\n",
    "            res_df = pd.concat([res_df, temp_df], ignore_index=True)\n",
    "        res_df = res_df.replace(\"Yes\", 1)\n",
    "        res_df = res_df.replace(\"No\", 0)\n",
    "        res_df = res_df.replace(\"None\", np.nan)\n",
    "        return res_df\n",
    "    \n",
    "    \n",
    "    def extract_bio(self):\n",
    "        bio = self.bio_df[[\"participant_id\", \"visit_name\", \"visit_month\", \"test_name\", \"test_value\"]]\n",
    "        res_df = pd.DataFrame({\"participant_id\": bio[\"participant_id\"].values.tolist(), \n",
    "                               \"visit_name\": bio[\"visit_name\"].values.tolist(),\n",
    "                               \"visit_month\": bio[\"visit_month\"].values.tolist(),\n",
    "                               \"variable\": bio[\"test_name\"].values.tolist(),\n",
    "                               \"var_type\": ['biospecimen'] * len(bio),\n",
    "                               \"source\": ['CSF_abeta_tau_ptau'] * len(bio),\n",
    "                               \"value\": bio[\"test_value\"].values.tolist()})\n",
    "        return res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progression representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the generated data will be fed to the LSTM model and conduct the progression representation lerning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import *\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup utility classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPMI_dataset(Dataset):\n",
    "    def __init__(self, new_dict):\n",
    "        filt_Pat = []\n",
    "        count = 0\n",
    "        for patient, mat in new_dict.items():\n",
    "            count +=1\n",
    "            if mat.shape[0] < 10:\n",
    "                num_row = 10-mat.shape[0]\n",
    "                add = np.zeros((num_row, mat.shape[1]))\n",
    "                new_dict[patient] = np.append(mat,add,0)\n",
    "\n",
    "            if mat.shape[0] > 10:\n",
    "                newmat = mat[:10, :]\n",
    "                new_dict[patient] = newmat\n",
    "        self.samples = new_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __featureNum__(self):\n",
    "        return self.__getitem__(0)[1].shape[1]\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        return [list(self.samples)[idx], list(self.samples.values())[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderRNN(nn.Module):\n",
    "    \n",
    "    SEED = 1234\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_size):\n",
    "        super(AutoEncoderRNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm_enc = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.lstm_dec = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.fc_dec = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        hiddens = []\n",
    "\n",
    "        # initialize h and c as 0\n",
    "        o_t = Variable(torch.zeros(self.batch_size, self.hidden_size)).to(self.device)\n",
    "        h_t = Variable(torch.zeros(self.batch_size, self.hidden_size)).to(self.device)\n",
    "\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # encoder\n",
    "        for i in range(seq_len):\n",
    "            x_t = x[:, i, :]\n",
    "            o_t, h_t = self.lstm_enc(x_t, (o_t, h_t))\n",
    "            hiddens += [h_t]\n",
    "\n",
    "        # hidden = h_t.data.clone()\n",
    "        hiddens = torch.stack(hiddens, 1).squeeze(2)\n",
    "        hiddens = torch.flip(hiddens, [1])\n",
    "\n",
    "        # decoder\n",
    "        x_r = Variable(torch.zeros(self.batch_size, self.input_size))\n",
    "        for j in range(seq_len):\n",
    "            o_t, h_t = self.lstm_dec(x_r, (o_t, h_t))\n",
    "            x_r = self.fc_dec(h_t)\n",
    "            outputs += [x_r]\n",
    "\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "\n",
    "        return hiddens, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train():\n",
    "    \n",
    "    SEED = 1234\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def __init__(self, Momentum=0.9, hidden_size=32, num_layers=1, batch_size=2, LR=0.001, sequence_length=10, nw=2):\n",
    "        self.Momentum = Momentum\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.LR = LR\n",
    "        self.sequence_length = sequence_length\n",
    "        self.nw = nw\n",
    "        \n",
    "    def train(self, sequence_data, save_path=None):\n",
    "        \n",
    "        dataset = PPMI_dataset(sequence_data)\n",
    "        input_size = dataset.__featureNum__()\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.nw, drop_last=True)\n",
    "        model = AutoEncoderRNN(input_size, self.hidden_size, self.num_layers, self.batch_size)\n",
    "        model = model.float()\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=self.LR, momentum=self.Momentum)\n",
    "        epochs = 100\n",
    "        \n",
    "        total_error = []\n",
    "        for epoch in range(epochs):\n",
    "            error = []\n",
    "            for i, data in enumerate(dataloader):\n",
    "                model = model.train()\n",
    "\n",
    "                patient, inputs = data[0], data[1]\n",
    "\n",
    "                enc, pred = model(inputs.float())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(inputs.float(), pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                error.append(loss.data.cpu().numpy())\n",
    "\n",
    "            print(\"Epoch: %s, loss: %s.\" % (epoch, np.mean(error)))\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                model.eval()\n",
    "                patient_encode = {}\n",
    "                patient_full_hidden = {}\n",
    "                with torch.no_grad():\n",
    "                    for j, data in enumerate(dataloader):\n",
    "                        patient, inputs = data[0], data[1].float().to(self.device)\n",
    "                        enc, _ = model(inputs)\n",
    "\n",
    "                        encoded = enc.data.cpu().numpy()\n",
    "                        vals = encoded[:, -1, :].reshape((self.batch_size, self.hidden_size))  # !!!! should be 0\n",
    "                        for z in range(self.batch_size):\n",
    "                            p = patient[z]\n",
    "                            v = vals[z, :]\n",
    "                            full_v = encoded[z, :, :]\n",
    "                            patient_encode[p] = v\n",
    "                            patient_full_hidden[p] = full_v\n",
    "\n",
    "                    if save_path is not None:\n",
    "                        if not os.path.exists(save_path):\n",
    "                            os.mkdir(save_path)\n",
    "                        with open(save_path + '/' + 'full_hidden_' + 'epoch_%s' % epoch + '.pkl', 'wb') as wf:\n",
    "                            pkl.dump(patient_full_hidden, wf)\n",
    "\n",
    "            total_error.append(np.mean(error))\n",
    "        plt.plot(total_error)\n",
    "        plt.savefig(save_path + '/' + 'loss.pdf')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup utility classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering():\n",
    "    \n",
    "    def __init__(self, feature_list, static_data, seq_data, data_path=None):\n",
    "        self.tables = get_var_table(feature_list, static_data, seq_data, data_path=None)\n",
    "        \n",
    "        \n",
    "    def UPDRS1(self, dataframe, seq_data, feature_map, visit):\n",
    "        target_vars = ['updrs1', 'hallucination', 'Apathy', 'Pain', 'Fatigue']\n",
    "        updrs1_idxs = [feature_map[ft] for ft in [\"code_upd2101_cognitive_impairment\", \"code_upd2102_hallucinations_and_psychosis\", \"code_upd2103_depressed_mood\", \"code_upd2104_anxious_mood\", \"code_upd2105_apathy\", \"code_upd2106_dopamine_dysregulation_syndrome_features\", \"code_upd2107_pat_quest_sleep_problems\", \"code_upd2108_pat_quest_daytime_sleepiness\", \"code_upd2109_pat_quest_pain_and_other_sensations\", \"code_upd2110_pat_quest_urinary_problems\", \"code_upd2111_pat_quest_constipation_problems\", \"code_upd2112_pat_quest_lightheadedness_on_standing\", \"code_upd2113_pat_quest_fatigue\"]]\n",
    "        hal_idx, apa_idx, pain_idx, fat_idx = feature_map[\"code_upd2102_hallucinations_and_psychosis\"], feature_map[\"code_upd2105_apathy\"], feature_map[\"code_upd2105_apathy\"], feature_map[\"code_upd2113_pat_quest_fatigue\"]\n",
    "\n",
    "        dataframe = dataframe.reindex(columns=list(dataframe.columns)+target_vars)\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            PATNO = row['participant_id']\n",
    "            if PATNO not in seq_data:\n",
    "                print(\"Error: participant_id %s not in sequence data!\" % PATNO)\n",
    "                return\n",
    "            data_mtx = seq_data[PATNO]\n",
    "            if visit >= len(data_mtx):\n",
    "                continue\n",
    "\n",
    "            updrs1_score = data_mtx[visit, updrs1_idxs].sum()\n",
    "            hal, apa, pain, fat = data_mtx[visit, hal_idx], data_mtx[visit, apa_idx], data_mtx[visit, pain_idx], data_mtx[visit, fat_idx]\n",
    "            dataframe.loc[idx, target_vars] = [updrs1_score, hal, apa, pain, fat]\n",
    "\n",
    "        print(\"UPDRS1 domain finished!\")\n",
    "        return dataframe\n",
    "\n",
    "    def Epworth(self, dataframe, seq_data, feature_map, visit):\n",
    "        target_vars = ['epworth']\n",
    "        epworth_idxs = [feature_map[ft] for ft in\n",
    "                       [\"code_ess0101_sitting_and_reading\", \"code_ess0102_watching_tv\", \"code_ess0103_sitting_inactive_in_public_place\", \"code_ess0104_passenger_in_car_for_hour\", \"code_ess0105_lying_down_to_rest_in_afternoon\", \"code_ess0106_sitting_and_talking_to_someone\", \"code_ess0107_sitting_after_lunch\", \"code_ess0108_car_stopped_in_traffic\"]]\n",
    "\n",
    "        dataframe = dataframe.reindex(columns=list(dataframe.columns)+target_vars)\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            PATNO = row['participant_id']\n",
    "            if PATNO not in seq_data:\n",
    "                print(\"Error: participant_id %s not in sequence data!\" % PATNO)\n",
    "                return\n",
    "            data_mtx = seq_data[PATNO]\n",
    "            if visit >= len(data_mtx):\n",
    "                continue\n",
    "\n",
    "            epworth_score = data_mtx[visit, epworth_idxs].sum()\n",
    "            dataframe.loc[idx, target_vars] = [epworth_score]\n",
    "\n",
    "        print(\"Epworth domain finished!\")\n",
    "        return dataframe\n",
    "\n",
    "    def MOCA(self, dataframe, seq_data, static_df, feature_map, visit):\n",
    "        target_vars = ['moca', 'moca_visuospatial', 'moca_naming', 'moca_attention',\n",
    "                       'moca_language', 'moca_delayed_recall']\n",
    "\n",
    "        moca_idxs = [feature_map[ft] for ft in ['moca_total_score']]\n",
    "        visu_idxs = [feature_map[ft] for ft in [\"moca01_alternating_trail_making\", \"moca02_visuoconstr_skills_cube\", \"moca03_visuoconstr_skills_clock_cont\", \"moca04_visuoconstr_skills_clock_num\", \"moca05_visuoconstr_skills_clock_hands\"]]\n",
    "        nam_idxs = [feature_map[ft] for ft in [\"moca06_naming_lion\", \"moca07_naming_rhino\", \"moca08_naming_camel\"]]\n",
    "        att_idxs = [feature_map[ft] for ft in [\"moca09_attention_forward_digit_span\", \"moca10_attention_backward_digit_span\", \"moca11_attention_vigilance\", \"moca12_attention_serial_7s\"]]\n",
    "        lang_idxs = [feature_map[ft] for ft in [\"moca13_sentence_repetition\", \"moca15_verbal_fluency\"]]\n",
    "        dere_idxs = [feature_map[ft] for ft in [\"moca17_delayed_recall_face\", \"moca18_delayed_recall_velvet\", \"moca19_delayed_recall_church\", \"moca20_delayed_recall_daisy\", \"moca21_delayed_recall_red\"]]\n",
    "\n",
    "        dataframe = dataframe.reindex(columns=list(dataframe.columns)+target_vars)\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            PATNO = row['participant_id']\n",
    "            if PATNO not in seq_data:\n",
    "                print(\"Error: PATNO %s not in sequence data!\" % PATNO)\n",
    "                return\n",
    "            data_mtx = seq_data[PATNO]\n",
    "            if visit >= len(data_mtx):\n",
    "                continue\n",
    "\n",
    "            EDUCYRS = static_df.loc[PATNO, 'education_level_years'] # education years\n",
    "\n",
    "            moca = data_mtx[visit, moca_idxs].sum()\n",
    "            visu = data_mtx[visit, visu_idxs].sum()\n",
    "            nam = data_mtx[visit, nam_idxs].sum()\n",
    "            att = data_mtx[visit, att_idxs].sum()\n",
    "            lang = data_mtx[visit, lang_idxs].sum()\n",
    "            dere = data_mtx[visit, dere_idxs].sum()\n",
    "\n",
    "            if EDUCYRS == \"Less than 12 years\" and moca < 30:\n",
    "                adj_moca = moca + 1\n",
    "            else:\n",
    "                adj_moca = moca\n",
    "\n",
    "            dataframe.loc[idx, target_vars] = [adj_moca, visu, nam, att, lang, dere]\n",
    "\n",
    "        print(\"MOCA domain finished!\")\n",
    "        return dataframe\n",
    "\n",
    "    def RBD(self, dataframe, seq_data, feature_map, visit):\n",
    "        target_vars = ['RBD']\n",
    "\n",
    "        term1_idxs = [feature_map[ft] for ft in [\"code_rbd01_vivid_dreams\", \"code_rbd02_aggressive_or_action_packed_dreams\", \"code_rbd03_nocturnal_behaviour\", \"code_rbd04_move_arms_legs_during_sleep\", \"code_rbd05_hurt_bed_partner\", \"code_rbd06_1_speaking_in_sleep\", \"code_rbd06_2_sudden_limb_movements\", \"code_rbd06_3_complex_movements\", \"code_rbd06_4_things_fell_down\", \"code_rbd07_my_movements_awake_me\", \"code_rbd08_remember_dreams\", \"code_rbd09_sleep_is_disturbed\"]]\n",
    "        term2_idxs = [feature_map[ft] for ft in [\"code_rbd10a_stroke\", \"code_rbd10b_head_trauma\", \"code_rbd10c_parkinsonism\", \"code_rbd10d_rls\", \"code_rbd10e_narcolepsy\", \"code_rbd10f_depression\", \"code_rbd10g_epilepsy\", \"code_rbd10h_brain_inflammatory_disease\", \"code_rbd10i_other\"]]\n",
    "\n",
    "        dataframe = dataframe.reindex(columns=list(dataframe.columns) + target_vars)\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            PATNO = row['participant_id']\n",
    "            if PATNO not in seq_data:\n",
    "                print(\"Error: PATNO %s not in sequence data!\" % PATNO)\n",
    "                return\n",
    "            data_mtx = seq_data[PATNO]\n",
    "            if visit >= len(data_mtx):\n",
    "                continue\n",
    "\n",
    "            term1 = data_mtx[visit, term1_idxs].sum()\n",
    "            term2 = data_mtx[visit, term2_idxs].sum()\n",
    "\n",
    "            rbd = term1\n",
    "            if term2 >= 1:\n",
    "                rbd += 1\n",
    "\n",
    "            dataframe.loc[idx, target_vars] = [rbd]\n",
    "\n",
    "        print(\"RBD domain finished!\")\n",
    "        return dataframe\n",
    "    \n",
    "    def UPDRS2(self, dataframe, seq_data, feature_map, visit):\n",
    "        target_vars = ['updrs2']\n",
    "        updrs2_idxs = [feature_map[ft] for ft in [\"code_upd2201_speech\", \"code_upd2202_saliva_and_drooling\", \"code_upd2203_chewing_and_swallowing\", \"code_upd2204_eating_tasks\", \"code_upd2205_dressing\", \"code_upd2206_hygiene\", \"code_upd2207_handwriting\", \"code_upd2208_doing_hobbies_and_other_activities\", \"code_upd2209_turning_in_bed\", \"code_upd2210_tremor\", \"code_upd2211_get_out_of_bed_car_or_deep_chair\", \"code_upd2212_walking_and_balance\", \"code_upd2213_freezing\"]]\n",
    "\n",
    "        dataframe = dataframe.reindex(columns=list(dataframe.columns)+target_vars)\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            PATNO = row['participant_id']\n",
    "            if PATNO not in seq_data:\n",
    "                print(\"Error: PATNO %s not in sequence data!\" % PATNO)\n",
    "                return\n",
    "            data_mtx = seq_data[PATNO]\n",
    "            if visit >= len(data_mtx):\n",
    "                continue\n",
    "\n",
    "            updrs2_score = data_mtx[visit, updrs2_idxs].sum()\n",
    "            dataframe.loc[idx, target_vars] = [updrs2_score]\n",
    "\n",
    "        print(\"UPDRS2 domain finished!\")\n",
    "        return dataframe\n",
    "\n",
    "    def UPDRS3(self, dataframe, seq_data, feature_map, visit):\n",
    "        target_vars = ['updrs3']\n",
    "        updrs3_idxs = [feature_map[ft] for ft in [\"code_upd2301_speech_problems\", \"code_upd2302_facial_expression\", \"code_upd2303a_rigidity_neck\", \"code_upd2303b_rigidity_rt_upper_extremity\", \"code_upd2303c_rigidity_left_upper_extremity\", \"code_upd2303d_rigidity_rt_lower_extremity\", \"code_upd2303e_rigidity_left_lower_extremity\", \"code_upd2304a_right_finger_tapping\", \"code_upd2304b_left_finger_tapping\", \"code_upd2305a_right_hand_movements\", \"code_upd2305b_left_hand_movements\", \"code_upd2306a_pron_sup_movement_right_hand\", \"code_upd2306b_pron_sup_movement_left_hand\", \"code_upd2307a_right_toe_tapping\", \"code_upd2307b_left_toe_tapping\", \"code_upd2308a_right_leg_agility\", \"code_upd2308b_left_leg_agility\", \"code_upd2309_arising_from_chair\", \"code_upd2310_gait\", \"code_upd2311_freezing_of_gait\", \"code_upd2312_postural_stability\", \"code_upd2313_posture\", \"code_upd2314_body_bradykinesia\", \"code_upd2315a_postural_tremor_of_right_hand\", \"code_upd2315b_postural_tremor_of_left_hand\", \"code_upd2316a_kinetic_tremor_of_right_hand\", \"code_upd2316b_kinetic_tremor_of_left_hand\", \"code_upd2317a_rest_tremor_amplitude_right_upper_extremity\", \"code_upd2317b_rest_tremor_amplitude_left_upper_extremity\", \"code_upd2317c_rest_tremor_amplitude_right_lower_extremity\", \"code_upd2317d_rest_tremor_amplitude_left_lower_extremity\", \"code_upd2317e_rest_tremor_amplitude_lip_or_jaw\", \"code_upd2318_consistency_of_rest_tremor\"]]\n",
    "\n",
    "        dataframe = dataframe.reindex(columns=list(dataframe.columns)+target_vars)\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            PATNO = row['participant_id']\n",
    "            if PATNO not in seq_data:\n",
    "                print(\"Error: PATNO %s not in sequence data!\" % PATNO)\n",
    "                return\n",
    "            data_mtx = seq_data[PATNO]\n",
    "            if visit >= len(data_mtx):\n",
    "                continue\n",
    "\n",
    "            updrs3_score = data_mtx[visit, updrs3_idxs].sum()\n",
    "            dataframe.loc[idx, target_vars] = [updrs3_score]\n",
    "\n",
    "        print(\"UPDRS3 domain finished!\")\n",
    "        return dataframe\n",
    "\n",
    "    def Schwab(self, dataframe, seq_data, feature_map, visit):\n",
    "        target_vars = ['Schwab']\n",
    "        schwab_idxs = [feature_map[ft] for ft in [\"mod_schwab_england_pct_adl_score\"]]\n",
    "\n",
    "        dataframe = dataframe.reindex(columns=list(dataframe.columns) + target_vars)\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            PATNO = row['participant_id']\n",
    "            if PATNO not in seq_data:\n",
    "                print(\"Error: PATNO %s not in sequence data!\" % PATNO)\n",
    "                return\n",
    "            data_mtx = seq_data[PATNO]\n",
    "            if visit >= len(data_mtx):\n",
    "                continue\n",
    "\n",
    "            schwab_score = data_mtx[visit, schwab_idxs].sum()\n",
    "            dataframe.loc[idx, target_vars] = [schwab_score]\n",
    "\n",
    "        print(\"Schwab domain finished!\")\n",
    "        return dataframe\n",
    "    \n",
    "    def Bio(self, dataframe, seq_data, feature_map, visit):\n",
    "        target_vars = [\"Tau\", \"Abeta\", \"p-Tau\", \"abeta_42_total_tau_ratio\"]\n",
    "\n",
    "        abeta_42_idxs = [feature_map[ft] for ft in [\"Abeta\"]]\n",
    "        p_tau181p_idxs = [feature_map[ft] for ft in [\"p-Tau\"]]\n",
    "        total_tau_idxs = [feature_map[ft] for ft in [\"Tau\"]]\n",
    "\n",
    "        dataframe = dataframe.reindex(columns=list(dataframe.columns) + target_vars)\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            PATNO = row['participant_id']\n",
    "            if PATNO not in seq_data:\n",
    "                print(\"Error: PATNO %s not in sequence data!\" % PATNO)\n",
    "                return\n",
    "            data_mtx = seq_data[PATNO]\n",
    "            if visit >= len(data_mtx):\n",
    "                continue\n",
    "\n",
    "            abeta_42 = data_mtx[visit, abeta_42_idxs].sum()\n",
    "            p_tau181p = data_mtx[visit, p_tau181p_idxs].sum()\n",
    "            total_tau = data_mtx[visit, total_tau_idxs].sum()\n",
    "\n",
    "            abeta_42_total_tau_ratio = abeta_42 / total_tau\n",
    "\n",
    "            dataframe.loc[idx, target_vars] = [abeta_42, p_tau181p, total_tau,\n",
    "                                               abeta_42_total_tau_ratio]\n",
    "        print(\"Biospecimen domain finished!\")\n",
    "        return dataframe\n",
    "    \n",
    "    def get_var_table(self, feature_list, static_data, seq_data, data_path=None):\n",
    "\n",
    "        feat_map = {}\n",
    "        idx = 0\n",
    "        for feat in feature_list:\n",
    "            feat_map[feat] = idx\n",
    "            idx += 1\n",
    "        \n",
    "        static_df = static_data.set_index('participant_id')\n",
    "\n",
    "        df = pd.DataFrame(data=list(seq_data.keys()), columns=['participant_id'])\n",
    "\n",
    "        # initialize\n",
    "        tables = {}\n",
    "        for v in range(35):\n",
    "            tables[v] = copy.deepcopy(df)\n",
    "\n",
    "        # update\n",
    "        if data_path is not None:   \n",
    "            folder = data_path + \"/visit_data/\"\n",
    "\n",
    "            if os.path.exists(folder) == False:\n",
    "                os.makedirs(folder)\n",
    "            for v in tables.keys():\n",
    "                tables[v] = self.UPDRS1(tables[v], seq_data, feat_map, v)\n",
    "                tables[v] = self.Epworth(tables[v], seq_data, feat_map, v)\n",
    "                tables[v] = self.MOCA(tables[v], seq_data, static_df, feat_map, v)  # adjusted\n",
    "                tables[v] = self.RBD(tables[v], seq_data, feat_map, v)\n",
    "\n",
    "                tables[v] = self.UPDRS2(tables[v], seq_data, feat_map, v)\n",
    "                tables[v] = self.UPDRS3(tables[v], seq_data, feat_map, v)\n",
    "                tables[v] = self.Schwab(tables[v], seq_data, feat_map, v)\n",
    "                tables[v] = self.Bio(tables[v], seq_data, feat_map, v)\n",
    "\n",
    "                tables[v].to_csv(folder+'/'+'V%02d.csv'%v, index=False)\n",
    "                print(\"------------ Visit %s finished! -------------\" % v)\n",
    "        else:\n",
    "            for v in tables.keys():\n",
    "                tables[v] = self.UPDRS1(tables[v], seq_data, feat_map, v)\n",
    "                tables[v] = self.Epworth(tables[v], seq_data, feat_map, v)\n",
    "                tables[v] = self.MOCA(tables[v], seq_data, static_df, feat_map, v)  # adjusted\n",
    "                tables[v] = self.RBD(tables[v], seq_data, feat_map, v)\n",
    "\n",
    "                tables[v] = self.UPDRS2(tables[v], seq_data, feat_map, v)\n",
    "                tables[v] = self.UPDRS3(tables[v], seq_data, feat_map, v)\n",
    "                tables[v] = self.Schwab(tables[v], seq_data, feat_map, v)\n",
    "                tables[v] = self.Bio(tables[v], seq_data, feat_map, v)\n",
    "                print(\"------------ Visit %s finished! -------------\" % v)\n",
    "        print(\"Generate all tables successfully!\")\n",
    "        return tables\n",
    "        \n",
    "    def clustering(self, clusterNum, random_state=0):\n",
    "        \"\"\"\n",
    "        :param data: input data\n",
    "        :param method: GMM, Agglomerative, KMeans\n",
    "        :param clusterNum: number of cluster\n",
    "\n",
    "        :return: list of labels\n",
    "        \"\"\"\n",
    "\n",
    "        if self.method == 'GMM':\n",
    "            model_gmm = mixture.GaussianMixture(n_components=clusterNum, covariance_type='tied', random_state=random_state)\n",
    "            model_gmm.fit(self.data)\n",
    "            labels = model_gmm.predict(self.data)\n",
    "            \n",
    "            return labels\n",
    "\n",
    "        if self.method == 'Agglomerative':\n",
    "            model_agg = cluster.AgglomerativeClustering(linkage='ward', n_clusters=clusterNum)\n",
    "            \"\"\"\n",
    "            linkage: “ward”, “complete”, “average”, “single”\n",
    "            \"\"\"\n",
    "            model_agg.fit(self.data)\n",
    "            labels = model_agg.labels_\n",
    "            \n",
    "            return labels\n",
    "\n",
    "        if self.method == 'KMeans':\n",
    "            model_km = cluster.KMeans(n_clusters=clusterNum, random_state=random_state)\n",
    "            model_km.fit(self.data)\n",
    "            labels = model_km.labels_\n",
    "            \n",
    "            return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1610/1610 [00:00<00:00, 5339.85rows/s]\n",
      "Downloading: 100%|██████████| 1610/1610 [00:00<00:00, 5288.36rows/s]\n",
      "Downloading: 100%|██████████| 4255/4255 [00:00<00:00, 10347.16rows/s]\n",
      "Downloading: 100%|██████████| 15971/15971 [00:04<00:00, 3260.10rows/s]\n",
      "Downloading: 100%|██████████| 16887/16887 [00:04<00:00, 3588.93rows/s]\n",
      "Downloading: 100%|██████████| 18624/18624 [00:12<00:00, 1465.72rows/s]\n",
      "Downloading: 100%|██████████| 10137/10137 [00:01<00:00, 6279.99rows/s]\n",
      "Downloading: 100%|██████████| 13127/13127 [00:00<00:00, 17760.25rows/s]\n",
      "Downloading: 100%|██████████| 9470/9470 [00:03<00:00, 2634.78rows/s]\n",
      "Downloading: 100%|██████████| 4741/4741 [00:00<00:00, 9700.58rows/s]\n",
      "Downloading: 100%|██████████| 9353/9353 [00:02<00:00, 4656.61rows/s]\n",
      "Downloading: 100%|██████████| 7674/7674 [00:03<00:00, 2182.79rows/s]\n",
      "Downloading: 100%|██████████| 17900/17900 [00:02<00:00, 6511.00rows/s]\n",
      "Downloading: 100%|██████████| 9385/9385 [00:00<00:00, 11540.95rows/s]\n"
     ]
    }
   ],
   "source": [
    "cohort_name = \"PPMI\"\n",
    "table_list = [\"demographics\", \"history\", \"updrs1\", \"updrs2\", \"updrs3\", \"updrs4\", \"schwab\", \"moca\", \"upsit\", \"ess\", \"rbd\",\n",
    "           \"med\", \"bio\"]\n",
    "DP = DataPreparation(cohort_name, table_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"processed_data/\"  # Files save path\n",
    "data, feature_list = DP.longitudinal_data(save_path = path)\n",
    "static_data = DP.static_data(save_path = path)\n",
    "visit_list = DP.visit_list(save_path = path)\n",
    "visit_info = DP.visit_info(data, visit_list, save_path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!! Patient %s has no BL information! PP-41385\n",
      "!!!! Patient %s has no BL information! PP-50546\n",
      "!!!! Patient %s has no BL information! PP-40163\n"
     ]
    }
   ],
   "source": [
    "patient_arrays, feature_median, patient_feature_median = DP.concatenate(data, visit_info, visit_list, feature_list, save_path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = DP.LOCF_FOCB_imputation(patient_arrays, patient_feature_median, feature_list, save_path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_score_normalized_data = DP.Z_score_normalization(imputed_data, feature_list, save_path = path)\n",
    "minmax_normalized_data = DP.minmax_normalization(imputed_data, feature_list, save_path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.7744246.\n",
      "Epoch: 1, loss: 0.7664474.\n",
      "Epoch: 2, loss: 0.75995284.\n",
      "Epoch: 3, loss: 0.74810725.\n",
      "Epoch: 4, loss: 0.7281516.\n",
      "Epoch: 5, loss: 0.7051745.\n",
      "Epoch: 6, loss: 0.6879359.\n",
      "Epoch: 7, loss: 0.67607975.\n",
      "Epoch: 8, loss: 0.66746813.\n",
      "Epoch: 9, loss: 0.6619425.\n",
      "Epoch: 10, loss: 0.6561092.\n",
      "Epoch: 11, loss: 0.6510414.\n",
      "Epoch: 12, loss: 0.64564294.\n",
      "Epoch: 13, loss: 0.64047694.\n",
      "Epoch: 14, loss: 0.6363029.\n",
      "Epoch: 15, loss: 0.6314501.\n",
      "Epoch: 16, loss: 0.6272899.\n",
      "Epoch: 17, loss: 0.6234004.\n",
      "Epoch: 18, loss: 0.6189703.\n",
      "Epoch: 19, loss: 0.61543775.\n",
      "Epoch: 20, loss: 0.6127099.\n",
      "Epoch: 21, loss: 0.6089978.\n",
      "Epoch: 22, loss: 0.6059923.\n",
      "Epoch: 23, loss: 0.6029846.\n",
      "Epoch: 24, loss: 0.5997982.\n",
      "Epoch: 25, loss: 0.5976755.\n",
      "Epoch: 26, loss: 0.59433633.\n",
      "Epoch: 27, loss: 0.591204.\n",
      "Epoch: 28, loss: 0.58900076.\n",
      "Epoch: 29, loss: 0.5872527.\n",
      "Epoch: 30, loss: 0.5848112.\n",
      "Epoch: 31, loss: 0.58161724.\n",
      "Epoch: 32, loss: 0.5794007.\n",
      "Epoch: 33, loss: 0.5776341.\n",
      "Epoch: 34, loss: 0.575514.\n",
      "Epoch: 35, loss: 0.5727346.\n",
      "Epoch: 36, loss: 0.5704056.\n",
      "Epoch: 37, loss: 0.5683192.\n",
      "Epoch: 38, loss: 0.5661292.\n",
      "Epoch: 39, loss: 0.56400996.\n",
      "Epoch: 40, loss: 0.56192136.\n",
      "Epoch: 41, loss: 0.56068736.\n",
      "Epoch: 42, loss: 0.5589064.\n",
      "Epoch: 43, loss: 0.55585176.\n",
      "Epoch: 60, loss: 0.5329268.\n",
      "Epoch: 61, loss: 0.5315733.\n",
      "Epoch: 62, loss: 0.53066236.\n",
      "Epoch: 63, loss: 0.5292488.\n",
      "Epoch: 64, loss: 0.5276611.\n",
      "Epoch: 65, loss: 0.52590656.\n",
      "Epoch: 66, loss: 0.52551603.\n",
      "Epoch: 67, loss: 0.5247871.\n",
      "Epoch: 68, loss: 0.5227043.\n",
      "Epoch: 69, loss: 0.5223263.\n",
      "Epoch: 70, loss: 0.52140766.\n",
      "Epoch: 71, loss: 0.5203524.\n",
      "Epoch: 72, loss: 0.5189355.\n",
      "Epoch: 73, loss: 0.5178061.\n",
      "Epoch: 74, loss: 0.51615655.\n",
      "Epoch: 75, loss: 0.5154295.\n",
      "Epoch: 76, loss: 0.51438.\n",
      "Epoch: 77, loss: 0.51327974.\n",
      "Epoch: 78, loss: 0.51148295.\n",
      "Epoch: 79, loss: 0.50982934.\n",
      "Epoch: 80, loss: 0.5099062.\n",
      "Epoch: 81, loss: 0.5088107.\n",
      "Epoch: 82, loss: 0.50777525.\n",
      "Epoch: 83, loss: 0.5068843.\n",
      "Epoch: 84, loss: 0.5049097.\n",
      "Epoch: 85, loss: 0.504776.\n",
      "Epoch: 86, loss: 0.5033268.\n",
      "Epoch: 87, loss: 0.50292015.\n",
      "Epoch: 88, loss: 0.5016047.\n",
      "Epoch: 89, loss: 0.50068.\n",
      "Epoch: 90, loss: 0.49928066.\n",
      "Epoch: 91, loss: 0.49856454.\n",
      "Epoch: 92, loss: 0.49789923.\n",
      "Epoch: 93, loss: 0.49677327.\n",
      "Epoch: 94, loss: 0.495707.\n",
      "Epoch: 95, loss: 0.49457467.\n",
      "Epoch: 96, loss: 0.493371.\n",
      "Epoch: 97, loss: 0.49279904.\n",
      "Epoch: 98, loss: 0.49203154.\n",
      "Epoch: 99, loss: 0.4911075.\n"
     ]
    }
   ],
   "source": [
    "with open(\"processed_data/sequence_data_Zscore.pkl\", \"rb\") as f:\n",
    "    sequence_data = pkl.load(f)\n",
    "    \n",
    "save_path = \"LSTM_Output\"\n",
    "Train = Train()\n",
    "Train.train(sequence_data, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
